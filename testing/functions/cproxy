import hashlib
import socks
import requests
from urllib.parse import urlparse
import urllib
import time
import socket
import socks
import signal
import sys
import os
from cacheout import lru
import signal

curr_url = None
curr_protocol = None

l = lru.LRUCache(maxsize=256, ttl=86400, timer=time.time, default=None) # use this cache to use least recently used cache policy (websites are cached in memory)
# ttl is num seconds in a day

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
server.bind(('', 8009))
server.listen(10) # we can change the backlog if necessary


def init_cache():
    if(os.path.exists('./cache')):
        for filename in os.listdir("./cache"):
            name = "./cache/" + filename
            f = open(name, 'rb')
            l.set(filename, f.read())
            f.close()
    else:
        os.mkdir('./cache')

init_cache()

def signal_handler(sig, frame):
    print("detected sigint")
    for h in l:
        filename = "./cache/" + h
        f = open(filename, 'wb+')
        f.write(l.get(h))
    l.clear()
    sys.exit(0)

def fetch_file(parsed_url: urllib.parse.ParseResult) -> bytes:
    h = hashlib.sha256()
    filename = None
    global curr_url
    global curr_protocol
    if(curr_url != None):
        filename = curr_url + parsed_url.path
    else:
        filename = parsed_url.netloc + parsed_url.path
    assert(filename != None)
    h.update(filename.encode())
    file_digest = h.hexdigest()
    content = l.get(file_digest)
    if(content == None):
        if(curr_url != None):
            print()
            print("curr_url is: " + curr_url)
            print()
        if(parsed_url.netloc == ''):
            url = curr_protocol + '://' + curr_url + '/' + parsed_url.path
        else:
            url = curr_protocol + '://' + parsed_url.netloc + parsed_url.path
        print(url)
        # set up tor connection 
        session = requests.session()
        session.proxies = {}
        session.proxies['http'] = 'socks5h://localhost:9050'
        session.proxies['https'] = 'socks5h://localhost:9050'

        content = session.get(url).content
        session.close()
        print("saving to cache")
        print(file_digest)
        l.set(file_digest, content)
        print()
        print("returning content")
        return content
    return content


def cproxy():
    print("starting cproxy")
    signal.signal(signal.SIGINT, signal_handler)
    while(True):
        client, addr = server.accept()
        print(addr)
        print()
        request = client.recv(1024)
        fields = request.split()
        print(fields)
        print()
        if(len(fields) <= 1):
            client.close()
            continue
        method = fields[0]
        filename = fields[1][1:] # has format -> /www.google.com
        print()
        print("filename is: " + filename.decode('utf-8'))
        print()
        parsed = urlparse(filename.decode('utf-8'))
        print(parsed)
        if(parsed.netloc != ''):
             global curr_url
             global curr_protocol
             curr_url = parsed.netloc
             curr_protocol = parsed.scheme
        content = fetch_file(parsed)
        if(content):
            print("sending 200")
            print()
            client.sendall(b'HTTP/1.0 200 OK\r\n\r\n' + content)
        else:
            print("sending 404")
            print()
            client.sendall(b'HTTP/1.0 404 NOT FOUND\r\n\r\n File Not Found')
        l.evict()
        client.close()

